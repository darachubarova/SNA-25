import json
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import spacy
import pickle
import os
import re
from collections import defaultdict
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer, AutoModel
import warnings

import matplotlib
matplotlib.use('Agg')  # –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –¥–∏—Å–ø–ª–µ—è
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.patches as mpatches
from collections import Counter

warnings.filterwarnings('ignore')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

def calculate_metrics(y_true, y_pred):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Precision, Recall, F1"""
    try:
        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        return precision, recall, f1
    except:
        return 0.0, 0.0, 0.0

class FeverDataProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö FEVER"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        try:
            self.nlp = spacy.load('en_core_web_sm')
        except OSError:
            print("–ú–æ–¥–µ–ª—å en_core_web_sm –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load('en_core_web_sm')
        
    def load_and_preprocess_data(self, max_samples=15000):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö FEVER"""
        print("–≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö FEVER")
        
        data = []
        label_mapping = {
            'SUPPORTS': 0,      # –ù–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
            'REFUTES': 1,       # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
            'NOT ENOUGH INFO': 2 # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ
        }
        
        sample_count = 0
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(tqdm(f, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", total=max_samples)):
                if i >= max_samples:
                    break
                    
                try:
                    item = json.loads(line.strip())
                    
                    if i < 3:
                        print(f"\n–û—Ç–ª–∞–¥–∫–∞ –∑–∞–ø–∏—Å–∏ {i}:")
                        print(f"–ö–ª—é—á–∏: {item.keys()}")
                        if 'evidence' in item:
                            print(f"Evidence —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {type(item['evidence'])}")
                            print(f"Evidence —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {item['evidence']}")
                    
                    if item['label'] in label_mapping:
                        evidence_text = self._extract_evidence_text(item.get('evidence', []))
                        
                        processed_item = {
                            'id': item.get('id', i),
                            'claim': item['claim'],
                            'evidence': evidence_text,
                            'label': label_mapping[item['label']],
                            'label_name': item['label']
                        }
                        data.append(processed_item)
                        sample_count += 1
                        
                except json.JSONDecodeError as e:
                    print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ —Å—Ç—Ä–æ–∫–µ {i}: {e}")
                    continue
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ {i}: {e}")
                    continue
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        if data:
            print("\n–ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            for i in range(min(2, len(data))):
                print(f"–ü—Ä–∏–º–µ—Ä {i+1}:")
                print(f"  Claim: {data[i]['claim'][:100]}...")
                print(f"  Evidence: {data[i]['evidence'][:100]}...")
                print(f"  Label: {data[i]['label_name']}")
                
        return data
    
    def _extract_evidence_text(self, evidence):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ evidence"""
        if not evidence:
            return ""
        
        evidence_texts = []
        
        try:
            if isinstance(evidence, list):
                for ev_group in evidence:
                    if isinstance(ev_group, list):
                        for ev_item in ev_group:
                            if isinstance(ev_item, list) and len(ev_item) >= 3:
                                evidence_texts.append(str(ev_item[2]))
                            elif isinstance(ev_item, str):
                                evidence_texts.append(ev_item)
                            elif isinstance(ev_item, dict) and 'text' in ev_item:
                                evidence_texts.append(ev_item['text'])
                    elif isinstance(ev_group, str):
                        evidence_texts.append(ev_group)
                    elif isinstance(ev_group, dict) and 'text' in ev_group:
                        evidence_texts.append(ev_group['text'])
            elif isinstance(evidence, str):
                evidence_texts.append(evidence)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è evidence: {e}")
            return ""
        
        return " ".join(evidence_texts) if evidence_texts else ""

class EntityExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è KG"""
    
    def __init__(self):
        try:
            self.nlp = spacy.load('en_core_web_sm')
        except OSError:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å en_core_web_sm –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load('en_core_web_sm')
            
        self.knowledge_graph = nx.DiGraph()
        self.entity_to_id = {}
        self.id_to_entity = {}
        self.relation_to_id = {}
        self.id_to_relation = {}
        
    def extract_entities_and_relations(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        print("–≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π")
        
        all_entities = set()
        all_relations = []
        entity_pairs = []
        
        processed_count = 0
        
        for item in tqdm(data, desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"):
            try:
                claim_entities = self._extract_entities_from_text(item['claim'])
                evidence_entities = self._extract_entities_from_text(item['evidence'])
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                claim_entities = self._filter_entities(claim_entities)
                evidence_entities = self._filter_entities(evidence_entities)
                
                all_entities.update(claim_entities)
                all_entities.update(evidence_entities)
                
                for claim_ent in claim_entities:
                    for ev_ent in evidence_entities:
                        if claim_ent != ev_ent:
                            relation = self._determine_relation(item['label'])
                            all_relations.append((claim_ent, relation, ev_ent))
                            entity_pairs.append((claim_ent, ev_ent, relation, item['label']))
                
                processed_count += 1
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∑–∞–ø–∏—Å–∏: {e}")
                continue
        
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –∑–∞–ø–∏—Å–µ–π")
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_entities)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π")
        
        if not all_entities:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π. –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏...")
            for item in data[:100]:
                words = item['claim'].split()[:5]
                for word in words:
                    if len(word) > 3:
                        all_entities.add(word.lower())
        
        self.entity_to_id = {ent: i for i, ent in enumerate(all_entities)}
        self.id_to_entity = {i: ent for ent, i in self.entity_to_id.items()}
        
        unique_relations = list(set([rel[1] for rel in all_relations])) if all_relations else ['supports', 'contradicts', 'neutral']
        self.relation_to_id = {rel: i for i, rel in enumerate(unique_relations)}
        self.id_to_relation = {i: rel for rel, i in self.relation_to_id.items()}
        
        edges_added = 0
        for head, relation, tail in tqdm(all_relations, desc="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ KG"):
            try:
                self.knowledge_graph.add_edge(
                    self.entity_to_id[head],
                    self.entity_to_id[tail],
                    relation=self.relation_to_id[relation]
                )
                edges_added += 1
            except Exception as e:
                continue
        
        print(f"–°–æ–∑–¥–∞–Ω –≥—Ä–∞—Ñ —Å {len(all_entities)} —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ {edges_added} –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏")

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        try:
            visualize_knowledge_graph(self, 'charts/knowledge_graph.png')
            plot_kg_statistics(self, 'charts/kg_statistics.png')
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")

        return entity_pairs
    
    def _extract_entities_from_text(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or not isinstance(text, str):
            return []
            
        try:
            doc = self.nlp(text[:15000])
            entities = []
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            for ent in doc.ents:
                if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT']:
                    clean_text = ent.text.lower().strip()
                    if len(clean_text) > 2 and not re.match(r'^\d+$', clean_text):
                        entities.append(clean_text)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
            for chunk in doc.noun_chunks:
                if chunk.root.pos_ in ['NOUN', 'PROPN'] and len(chunk.text) > 2:
                    clean_text = chunk.root.lemma_.lower()
                    if clean_text.isalpha() and len(clean_text) > 3:
                        entities.append(clean_text)
                        
            return list(set(entities))
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            words = text.split()
            return [word.lower() for word in words if len(word) > 3 and word.isalpha()]
    
    def _filter_entities(self, entities):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not entities:
            return []
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö –∏–ª–∏ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        stop_entities = {'thing', 'something', 'nothing', 'anything'}
        filtered = [e for e in entities if e not in stop_entities and len(e.split()) <= 3]
        return filtered[:10]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —à—É–º–∞
    
    def _determine_relation(self, label):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∫–∏"""
        if label == 0:  # SUPPORTS
            return 'supports'
        elif label == 1:  # REFUTES
            return 'contradicts'
        else:  # NOT ENOUGH INFO
            return 'neutral'

class KGEmbedding(nn.Module):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (DistMult)"""
    
    def __init__(self, num_entities, num_relations, embed_dim=200):
        super(KGEmbedding, self).__init__()
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embed_dim = embed_dim
        
        self.entity_embeddings = nn.Embedding(num_entities, embed_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embed_dim)
        
        nn.init.xavier_uniform_(self.entity_embeddings.weight)
        nn.init.xavier_uniform_(self.relation_embeddings.weight)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.normalize_entities()
        
    def normalize_entities(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        with torch.no_grad():
            self.entity_embeddings.weight.div_(
                torch.norm(self.entity_embeddings.weight, dim=1, keepdim=True).clamp(min=1e-12)
            )

    def forward(self, heads, relations, tails):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∞ –¥–ª—è —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ (TransE –≤–º–µ—Å—Ç–æ DistMult)"""
        head_embeds = self.entity_embeddings(heads)
        relation_embeds = self.relation_embeddings(relations)
        tail_embeds = self.entity_embeddings(tails)
    
        # TransE: h + r ‚âà t, score = -||h + r - t||
        scores = head_embeds + relation_embeds - tail_embeds
        distances = torch.norm(scores, p=2, dim=1)
        return distances  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)

class ContradictionDataset(Dataset):
    """Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π"""
    
    def __init__(self, data, tokenizer, entity_extractor, kg_model, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.entity_extractor = entity_extractor
        self.kg_model = kg_model
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ claim –∏ evidence
        claim = str(item.get('claim', ''))
        evidence = str(item.get('evidence', ''))
        text = f"{claim} [SEP] {evidence}"
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        claim_entities = self.entity_extractor._extract_entities_from_text(claim)
        evidence_entities = self.entity_extractor._extract_entities_from_text(evidence)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_ids = []
        for entity in set(claim_entities + evidence_entities):
            if entity in self.entity_extractor.entity_to_id:
                entity_ids.append(self.entity_extractor.entity_to_id[entity])
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ)
        if entity_ids:
            entity_ids_tensor = torch.tensor(entity_ids, dtype=torch.long).to(device)
            with torch.no_grad():
                entity_embeds = self.kg_model.entity_embeddings(entity_ids_tensor)
                # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –ø–æ –æ–±—Ä–∞—Ç–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
                weights = torch.ones(len(entity_ids), device=device) / (len(entity_ids) + 1e-6)
                kg_embedding = torch.sum(entity_embeds * weights.unsqueeze(1), dim=0)
        else:
            kg_embedding = torch.zeros(self.kg_model.embed_dim).to(device)
            
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(item['label'], dtype=torch.long),
            'kg_embedding': kg_embedding
        }

class ContradictionClassifier(nn.Module):
    """–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å –∫—Ä–æ—Å—Å-–∞—Ç—Ç–µ–Ω—à–µ–Ω–æ–º"""
    
    def __init__(self, pretrained_model_name, num_classes=3, kg_embed_dim=200):
        super(ContradictionClassifier, self).__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.3)
        
        bert_dim = self.bert.config.hidden_size
        self.kg_projection = nn.Linear(kg_embed_dim, bert_dim)
        
        # –ö—Ä–æ—Å—Å-–∞—Ç—Ç–µ–Ω—à–µ–Ω —Å–ª–æ–π
        self.cross_attention = nn.MultiheadAttention(bert_dim, num_heads=8)
        
        self.classifier = nn.Sequential(
            nn.Linear(bert_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, input_ids, attention_mask, kg_embeddings=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]
        
        if kg_embeddings is not None:
            # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º KG —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ BERT
            kg_projected = self.kg_projection(kg_embeddings)  # [batch_size, hidden_size]
            kg_projected = kg_projected.unsqueeze(1)  # [batch_size, 1, hidden_size]
            
            # –ö—Ä–æ—Å—Å-–∞—Ç—Ç–µ–Ω—à–µ–Ω –º–µ–∂–¥—É sequence_output –∏ kg_projected
            sequence_output = sequence_output.transpose(0, 1)  # [seq_len, batch_size, hidden_size]
            attn_output, _ = self.cross_attention(
                kg_projected.transpose(0, 1),  # query: [1, batch_size, hidden_size]
                sequence_output,               # key: [seq_len, batch_size, hidden_size]
                sequence_output               # value: [seq_len, batch_size, hidden_size]
            )
            attn_output = attn_output.transpose(0, 1).squeeze(1)  # [batch_size, hidden_size]
            
            combined = self.dropout(attn_output)
        else:
            combined = self.dropout(pooled_output)
            
        logits = self.classifier(combined)
        return logits

class SimpleContradictionClassifier(nn.Module):
    """–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –±–µ–∑ KG"""
    
    def __init__(self, pretrained_model_name, num_classes=3):
        super(SimpleContradictionClassifier, self).__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.3)
        
        bert_dim = self.bert.config.hidden_size
        self.classifier = nn.Sequential(
            nn.Linear(bert_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

def visualize_knowledge_graph(entity_extractor, save_path='charts/kg_visualization.png', max_nodes=100):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""
    print("üé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
    
    plt.figure(figsize=(15, 12))
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    G = entity_extractor.knowledge_graph
    if len(G.nodes()) > max_nodes:
        # –ë–µ—Ä–µ–º —É–∑–ª—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π
        node_degrees = dict(G.degree())
        top_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:max_nodes]
        top_node_ids = [node[0] for node in top_nodes]
        G = G.subgraph(top_node_ids)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ layout
    if len(G.nodes()) > 0:
        try:
            pos = nx.spring_layout(G, k=3, iterations=50)
        except:
            pos = nx.random_layout(G)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–≤–µ—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        relation_colors = {0: 'green', 1: 'red', 2: 'gray'}  # supports, contradicts, neutral
        
        # –†–∏—Å–æ–≤–∞–Ω–∏–µ —Ä—ë–±–µ—Ä —Å —Ä–∞–∑–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏
        for edge in G.edges(data=True):
            relation = edge[2].get('relation', 2)
            color = relation_colors.get(relation, 'gray')
            nx.draw_networkx_edges(G, pos, edgelist=[(edge[0], edge[1])],
                                 edge_color=color, alpha=0.6, width=1.5)
        
        # –†–∏—Å–æ–≤–∞–Ω–∏–µ —É–∑–ª–æ–≤
        node_sizes = [300 + G.degree(node) * 50 for node in G.nodes()]
        nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                             node_size=node_sizes, alpha=0.8)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –≤–∞–∂–Ω—ã—Ö —É–∑–ª–æ–≤
        important_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)#[:20]
        labels = {}
        for node_id, _ in important_nodes:
            if node_id in entity_extractor.id_to_entity:
                entity_name = entity_extractor.id_to_entity[node_id]
                labels[node_id] = entity_name[:15] + "..." if len(entity_name) > 15 else entity_name
        
        nx.draw_networkx_labels(G, pos, labels, font_size=8)
        
        # –õ–µ–≥–µ–Ω–¥–∞
        legend_elements = [
            mpatches.Patch(color='green', label='Supports'),
            mpatches.Patch(color='red', label='Contradicts'),
            mpatches.Patch(color='gray', label='Neutral')
        ]
        plt.legend(handles=legend_elements, loc='upper right')
        
        plt.title(f'Knowledge Graph Visualization\n'
                 f'Nodes: {len(G.nodes())}, Edges: {len(G.edges())}',
                 fontsize=14, fontweight='bold')
    else:
        plt.text(0.5, 0.5, 'No graph data available',
                ha='center', va='center', fontsize=16)
        plt.title('Knowledge Graph Visualization', fontsize=14)
    
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")

def plot_training_curves(train_losses_kg, val_losses_kg, train_accs_kg, val_accs_kg,
                        train_losses_no_kg, val_losses_no_kg, train_accs_no_kg, val_accs_no_kg,
                        train_precision_kg, val_precision_kg, train_recall_kg, val_recall_kg, train_f1_kg, val_f1_kg,
                        train_precision_no_kg, val_precision_no_kg, train_recall_no_kg, val_recall_no_kg, train_f1_no_kg, val_f1_no_kg,
                        save_path='charts/training_curves_detailed.png'):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    
    fig, axes = plt.subplots(3, 2, figsize=(18, 15))
    epochs = range(1, len(train_losses_kg) + 1)
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏ —Å KG
    axes[0, 0].plot(epochs, train_losses_kg, 'b-', label='Training Loss (KG)', linewidth=2, marker='o')
    axes[0, 0].plot(epochs, val_losses_kg, 'r-', label='Validation Loss (KG)', linewidth=2, marker='s')
    axes[0, 0].set_title('Model with Knowledge Graph - Loss', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏ –±–µ–∑ KG
    axes[0, 1].plot(epochs, train_losses_no_kg, 'b--', label='Training Loss (No KG)', linewidth=2, marker='o')
    axes[0, 1].plot(epochs, val_losses_no_kg, 'r--', label='Validation Loss (No KG)', linewidth=2, marker='s')
    axes[0, 1].set_title('Model without Knowledge Graph - Loss', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
    axes[1, 0].plot(epochs, train_accs_kg, 'b-', label='Training Accuracy (KG)', linewidth=2, marker='o')
    axes[1, 0].plot(epochs, val_accs_kg, 'r-', label='Validation Accuracy (KG)', linewidth=2, marker='s')
    axes[1, 0].plot(epochs, train_accs_no_kg, 'b--', label='Training Accuracy (No KG)', linewidth=2, marker='^')
    axes[1, 0].plot(epochs, val_accs_no_kg, 'r--', label='Validation Accuracy (No KG)', linewidth=2, marker='d')
    axes[1, 0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Accuracy (%)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ Precision –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
    axes[1, 1].plot(epochs, train_precision_kg, 'g-', label='Training Precision (KG)', linewidth=2, marker='o')
    axes[1, 1].plot(epochs, val_precision_kg, 'orange', label='Validation Precision (KG)', linewidth=2, marker='s')
    axes[1, 1].plot(epochs, train_precision_no_kg, 'g--', label='Training Precision (No KG)', linewidth=2, marker='^')
    axes[1, 1].plot(epochs, val_precision_no_kg, color='orange', linestyle='--', label='Validation Precision (No KG)', linewidth=2, marker='d')
    axes[1, 1].set_title('Precision Comparison', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Precision')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ Recall –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
    axes[2, 0].plot(epochs, train_recall_kg, 'purple', label='Training Recall (KG)', linewidth=2, marker='o')
    axes[2, 0].plot(epochs, val_recall_kg, 'brown', label='Validation Recall (KG)', linewidth=2, marker='s')
    axes[2, 0].plot(epochs, train_recall_no_kg, 'purple', linestyle='--', label='Training Recall (No KG)', linewidth=2, marker='^')
    axes[2, 0].plot(epochs, val_recall_no_kg, 'brown', linestyle='--', label='Validation Recall (No KG)', linewidth=2, marker='d')
    axes[2, 0].set_title('Recall Comparison', fontsize=14, fontweight='bold')
    axes[2, 0].set_xlabel('Epoch')
    axes[2, 0].set_ylabel('Recall')
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ F1-Score –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
    axes[2, 1].plot(epochs, train_f1_kg, 'teal', label='Training F1 (KG)', linewidth=2, marker='o')
    axes[2, 1].plot(epochs, val_f1_kg, 'navy', label='Validation F1 (KG)', linewidth=2, marker='s')
    axes[2, 1].plot(epochs, train_f1_no_kg, 'teal', linestyle='--', label='Training F1 (No KG)', linewidth=2, marker='^')
    axes[2, 1].plot(epochs, val_f1_no_kg, 'navy', linestyle='--', label='Validation F1 (No KG)', linewidth=2, marker='d')
    axes[2, 1].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')
    axes[2, 1].set_xlabel('Epoch')
    axes[2, 1].set_ylabel('F1-Score')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    pd.DataFrame([train_losses_kg, val_losses_kg, train_accs_kg, val_accs_kg,
                        train_losses_no_kg, val_losses_no_kg, train_accs_no_kg, val_accs_no_kg,
                        train_precision_kg, val_precision_kg, train_recall_kg, val_recall_kg, train_f1_kg, val_f1_kg,
                        train_precision_no_kg, val_precision_no_kg, train_recall_no_kg, val_recall_no_kg, train_f1_no_kg, val_f1_no_kg]).to_csv('charts/res.csv')
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: charts/res.csv")
    

def plot_metrics_summary(metrics_kg, metrics_no_kg, save_path='charts/metrics_summary.png'):
    """–°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    print("üìà –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ –º–µ—Ç—Ä–∏–∫...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    # –ë–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    x = np.arange(len(metrics_names))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, metrics_kg, width, label='With KG', color='skyblue', alpha=0.8)
    bars2 = ax1.bar(x + width/2, metrics_no_kg, width, label='Without KG', color='lightcoral', alpha=0.8)
    
    ax1.set_xlabel('Metrics')
    ax1.set_ylabel('Score')
    ax1.set_title('Model Performance Comparison', fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics_names)
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –±–∞—Ä–∞—Ö
    for bar in bars1:
        height = bar.get_height()
        ax1.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontsize=10)
    
    for bar in bars2:
        height = bar.get_height()
        ax1.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontsize=10)
    
    # –†–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
    angles += angles[:1]  # –ó–∞–º—ã–∫–∞–Ω–∏–µ –∫—Ä—É–≥–∞
    
    metrics_kg_radar = metrics_kg + [metrics_kg[0]]  # –ó–∞–º—ã–∫–∞–Ω–∏–µ –¥–ª—è —Ä–∞–¥–∞—Ä–∞
    metrics_no_kg_radar = metrics_no_kg + [metrics_no_kg[0]]
    
    ax2 = plt.subplot(122, projection='polar')
    ax2.plot(angles, metrics_kg_radar, 'o-', linewidth=2, label='With KG', color='blue')
    ax2.fill(angles, metrics_kg_radar, alpha=0.25, color='blue')
    ax2.plot(angles, metrics_no_kg_radar, 'o-', linewidth=2, label='Without KG', color='red')
    ax2.fill(angles, metrics_no_kg_radar, alpha=0.25, color='red')
    
    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(metrics_names)
    ax2.set_ylim(0, 1)
    ax2.set_title('Performance Radar Chart', fontweight='bold', pad=20)
    ax2.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")

def plot_confusion_matrices(y_true, y_pred_kg, y_pred_no_kg, class_names, save_path='charts/confusion_matrices.png'):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å KG
    cm_kg = confusion_matrix(y_true, y_pred_kg)
    sns.heatmap(cm_kg, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'}, ax=ax1)
    
    accuracy_kg = np.trace(cm_kg) / np.sum(cm_kg)
    ax1.set_title(f'Model with KG\nAccuracy: {accuracy_kg:.3f}', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Predicted Label', fontsize=12)
    ax1.set_ylabel('True Label', fontsize=12)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ –±–µ–∑ KG
    cm_no_kg = confusion_matrix(y_true, y_pred_no_kg)
    sns.heatmap(cm_no_kg, annot=True, fmt='d', cmap='Oranges',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'}, ax=ax2)
    
    accuracy_no_kg = np.trace(cm_no_kg) / np.sum(cm_no_kg)
    ax2.set_title(f'Model without KG\nAccuracy: {accuracy_no_kg:.3f}', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Predicted Label', fontsize=12)
    ax2.set_ylabel('True Label', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")

def plot_kg_statistics(entity_extractor, save_path='charts/kg_statistics.png'):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""
    print("üìà –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
    
    G = entity_extractor.knowledge_graph
    
    if len(G.nodes()) == 0:
        plt.figure(figsize=(10, 6))
        plt.text(0.5, 0.5, 'No graph data available', ha='center', va='center', fontsize=16)
        plt.title('Knowledge Graph Statistics')
        plt.axis('off')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        return
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π —É–∑–ª–æ–≤
    degrees = [G.degree(n) for n in G.nodes()]
    ax1.hist(degrees, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_title('Node Degree Distribution')

class ContradictionDetectionPipeline:
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        self.device = device
        self.data_processor = FeverDataProcessor(data_path)
        self.entity_extractor = EntityExtractor()
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        
    def run_full_pipeline(self, max_samples=5000):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print("–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π\n")
        
        # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data = self.data_processor.load_and_preprocess_data(max_samples)
        
        if len(data) == 0:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
            return None, None
        
        # –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ KG
        entity_pairs = self.entity_extractor.extract_entities_and_relations(data)
        
        # –≠—Ç–∞–ø 3: –û–±—É—á–µ–Ω–∏–µ KG —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        kg_model = self.train_kg_embeddings()
        
        # –≠—Ç–∞–ø 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("–≠—Ç–∞–ø 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, stratify=[item['label'] for item in data])
        
        train_dataset = ContradictionDataset(train_data, self.tokenizer, self.entity_extractor, kg_model)
        val_dataset = ContradictionDataset(val_data, self.tokenizer, self.entity_extractor, kg_model)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(train_data)} –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, {len(val_data)} –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        
        # –≠—Ç–∞–ø 5: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_with_kg, model_without_kg = self.train_classification_model(train_loader, val_loader, kg_model)
        
        # –≠—Ç–∞–ø 6: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
        accuracy_kg, accuracy_no_kg, report = self.evaluate_model(model_with_kg, model_without_kg, val_loader)
        
        return model_with_kg, model_without_kg
    
    def train_kg_embeddings(self):
        """–û–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""
        print("–≠—Ç–∞–ø 3: –û–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π")
    
        num_entities = len(self.entity_extractor.entity_to_id)
        num_relations = len(self.entity_extractor.relation_to_id)
    
        if num_entities == 0 or num_relations == 0:
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–ª–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è KG")
            kg_model = KGEmbedding(max(1, num_entities), max(1, num_relations)).to(self.device)
            return kg_model
        
        kg_model = KGEmbedding(num_entities, num_relations).to(self.device)
        optimizer = optim.Adam(kg_model.parameters(), lr=0.001, weight_decay=1e-5)  # –ú–µ–Ω—å—à–µ LR
    
        edges = list(self.entity_extractor.knowledge_graph.edges(data=True))
    
        if len(edges) == 0:
            print("–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç")
            return kg_model
        
        kg_model.train()
        num_epochs = 20
    
        for epoch in tqdm(range(num_epochs), desc="–û–±—É—á–µ–Ω–∏–µ KG —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"):
            total_loss = 0
            batch_size = 256  # –£–º–µ–Ω—å—à–∏–ª–∏ batch size
            batch_count = 0
            valid_batches = 0
        
            for i in range(0, len(edges), batch_size):
                batch_edges = edges[i:i+batch_size]
            
                if len(batch_edges) == 0:
                    continue
                
                try:
                    heads = torch.tensor([edge[0] for edge in batch_edges], dtype=torch.long).to(self.device)
                    tails = torch.tensor([edge[1] for edge in batch_edges], dtype=torch.long).to(self.device)
                    relations = torch.tensor([edge[2]['relation'] for edge in batch_edges], dtype=torch.long).to(self.device)
                
                    optimizer.zero_grad()
                
                    # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                    pos_scores = kg_model(heads, relations, tails)
                
                    # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                    neg_tails = torch.randint(0, num_entities, tails.shape).to(self.device)
                    neg_heads = torch.randint(0, num_entities, heads.shape).to(self.device)
                
                    neg_scores_tails = kg_model(heads, relations, neg_tails, negative=True)
                    neg_scores_heads = kg_model(neg_heads, relations, tails, negative=True)
                
                    # –°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
                    pos_scores = torch.clamp(pos_scores, 1e-7, 1-1e-7)
                    neg_scores_tails = torch.clamp(neg_scores_tails, 1e-7, 1-1e-7)
                    neg_scores_heads = torch.clamp(neg_scores_heads, 1e-7, 1-1e-7)
                
                    loss = (-torch.mean(torch.log(pos_scores))
                        - 0.5 * torch.mean(torch.log(1 - neg_scores_tails))
                        - 0.5 * torch.mean(torch.log(1 - neg_scores_heads)))
                
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ø–æ—Ç–µ—Ä—è –≤ —ç–ø–æ—Ö–µ {epoch}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á")
                        continue
                    
                    loss.backward()
                
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(kg_model.parameters(), max_norm=1.0)
                
                    optimizer.step()
                    kg_model.normalize_entities()
                
                    total_loss += loss.item()
                    valid_batches += 1
                
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ: {e}")
                    continue
                
                batch_count += 1
        
            if valid_batches > 0:
                avg_loss = total_loss / valid_batches
                if epoch % 10 == 0:
                    print(f"–≠–ø–æ—Ö–∞ {epoch}, —Å—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è: {avg_loss:.4f}, –≤–∞–ª–∏–¥–Ω—ã—Ö –±–∞—Ç—á–µ–π: {valid_batches}/{batch_count}")
            else:
                print(f"‚ö†Ô∏è –≠–ø–æ—Ö–∞ {epoch}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –±–∞—Ç—á–µ–π")
    
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ KG –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return kg_model
    
    def train_classification_model(self, train_loader, val_loader, kg_model):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å KG –∏ –±–µ–∑ KG)"""
        print("–≠—Ç–∞–ø 5: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π")
        
        model_with_kg = ContradictionClassifier('bert-base-uncased').to(self.device)
        model_without_kg = SimpleContradictionClassifier('bert-base-uncased').to(self.device)
        
        optimizer_kg = optim.AdamW(model_with_kg.parameters(), lr=2e-5)
        optimizer_no_kg = optim.AdamW(model_without_kg.parameters(), lr=2e-5)
        criterion = nn.CrossEntropyLoss()
        
        # –î–û–ë–ê–í–ò–¢–¨ –≠–¢–ò –ü–ï–†–ï–ú–ï–ù–ù–´–ï –î–õ–Ø –ì–†–ê–§–ò–ö–û–í:
        train_losses_kg = []
        val_losses_kg = []
        train_accs_kg = []
        val_accs_kg = []
        train_losses_no_kg = []
        val_losses_no_kg = []
        train_accs_no_kg = []
        val_accs_no_kg = []
        
        # –î–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è (–¥–æ–±–∞–≤–∏—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º)
        train_precision_kg = []
        val_precision_kg = []
        train_recall_kg = []
        val_recall_kg = []
        train_f1_kg = []
        val_f1_kg = []

        train_precision_no_kg = []
        val_precision_no_kg = []
        train_recall_no_kg = []
        val_recall_no_kg = []
        train_f1_no_kg = []
        val_f1_no_kg = []
        
        num_epochs = 2
        best_val_acc_kg = 0
        best_val_acc_no_kg = 0
        
        for epoch in range(num_epochs):
            print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}")
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å KG
            model_with_kg.train()
            train_loss_kg = 0
            train_correct_kg = 0
            train_total_kg = 0
            
            train_pbar = tqdm(train_loader, desc=f"–û–±—É—á–µ–Ω–∏–µ (KG) —ç–ø–æ—Ö–∞ {epoch + 1}")
            # –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è KG –º–æ–¥–µ–ª–∏, –¥–æ–±–∞–≤–∏—Ç—å:
            epoch_train_losses_kg = []
            epoch_train_preds_kg = []
            epoch_train_labels_kg = []
            for batch_idx, batch in enumerate(train_pbar):
                try:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['label'].to(self.device)
                    kg_embeddings = batch['kg_embedding'].to(self.device)
                    
                    optimizer_kg.zero_grad()
                    
                    outputs = model_with_kg(input_ids, attention_mask, kg_embeddings)
                    loss = criterion(outputs, labels)
                    epoch_train_losses_kg.append(loss.item())  # ‚Üê –î–û–ë–ê–í–ò–¢–¨
                    loss.backward()
                    optimizer_kg.step()
                    
                    train_loss_kg += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    epoch_train_preds_kg.extend(predicted.cpu().numpy())
                    epoch_train_labels_kg.extend(labels.cpu().numpy())
                    train_total_kg += labels.size(0)
                    train_correct_kg += (predicted == labels).sum().item()
                    
                    train_pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{100 * train_correct_kg / train_total_kg:.2f}%'
                    })
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ (KG) {batch_idx}: {e}")
                    continue
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ KG
            model_without_kg.train()
            train_loss_no_kg = 0
            train_correct_no_kg = 0
            train_total_no_kg = 0
            
            train_pbar_no_kg = tqdm(train_loader, desc=f"–û–±—É—á–µ–Ω–∏–µ (–±–µ–∑ KG) —ç–ø–æ—Ö–∞ {epoch + 1}")
            # –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±–µ–∑ KG, –¥–æ–±–∞–≤–∏—Ç—å:
            epoch_train_preds_no_kg = []
            epoch_train_labels_no_kg = []
            epoch_train_losses_no_kg = []
            for batch_idx, batch in enumerate(train_pbar_no_kg):
                try:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['label'].to(self.device)
                    
                    optimizer_no_kg.zero_grad()
                    
                    outputs = model_without_kg(input_ids, attention_mask)
                    loss = criterion(outputs, labels)
                    epoch_train_losses_no_kg.append(loss.item())  # ‚Üê –î–û–ë–ê–í–ò–¢–¨
                    
                    loss.backward()
                    optimizer_no_kg.step()
                    
                    train_loss_no_kg += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    epoch_train_preds_no_kg.extend(predicted.cpu().numpy())
                    epoch_train_labels_no_kg.extend(labels.cpu().numpy())
                    train_total_no_kg += labels.size(0)
                    train_correct_no_kg += (predicted == labels).sum().item()
                    
                    train_pbar_no_kg.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{100 * train_correct_no_kg / train_total_no_kg:.2f}%'
                    })
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ (–±–µ–∑ KG) {batch_idx}: {e}")
                    continue
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model_with_kg.eval()
            model_without_kg.eval()
            val_loss_kg = 0
            val_correct_kg = 0
            val_total_kg = 0
            val_loss_no_kg = 0
            val_correct_no_kg = 0
            val_total_no_kg = 0
            
            # –í —Ü–∏–∫–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–±–∞–≤–∏—Ç—å:
            epoch_val_losses_kg = []
            epoch_val_losses_no_kg = []
            epoch_val_preds_kg = []
            epoch_val_labels_kg = []
            epoch_val_preds_no_kg = []
            epoch_val_labels_no_kg = []
            
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc=f"–í–∞–ª–∏–¥–∞—Ü–∏—è —ç–ø–æ—Ö–∞ {epoch + 1}")
                for batch in val_pbar:
                    try:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        labels = batch['label'].to(self.device)
                        kg_embeddings = batch['kg_embedding'].to(self.device)
                        
                        outputs_kg = model_with_kg(input_ids, attention_mask, kg_embeddings)
                        loss_kg = criterion(outputs_kg, labels)
                        epoch_val_losses_kg.append(loss_kg.item())  # ‚Üê –î–û–ë–ê–í–ò–¢–¨
                        
                        val_loss_kg += loss_kg.item()
                        _, predicted_kg = torch.max(outputs_kg.data, 1)
                        val_total_kg += labels.size(0)
                        val_correct_kg += (predicted_kg == labels).sum().item()
                        
                        outputs_no_kg = model_without_kg(input_ids, attention_mask)
                        loss_no_kg = criterion(outputs_no_kg, labels)
                        epoch_val_losses_no_kg.append(loss_no_kg.item())  # ‚Üê –î–û–ë–ê–í–ò–¢–¨
                        
                        val_loss_no_kg += loss_no_kg.item()
                        _, predicted_no_kg = torch.max(outputs_no_kg.data, 1)
                        val_total_no_kg += labels.size(0)
                        val_correct_no_kg += (predicted_no_kg == labels).sum().item()
                        
                        epoch_val_preds_kg.extend(predicted_kg.cpu().numpy())
                        epoch_val_labels_kg.extend(labels.cpu().numpy())
                        epoch_val_preds_no_kg.extend(predicted_no_kg.cpu().numpy())
                        epoch_val_labels_no_kg.extend(labels.cpu().numpy())
                        
                    except Exception as e:
                        continue
                        
                    val_pbar.set_postfix({
                        'Loss (KG)': f'{loss_kg.item():.4f}',
                        'Acc (KG)': f'{100 * val_correct_kg / val_total_kg:.2f}%',
                        'Loss (No KG)': f'{loss_no_kg.item():.4f}',
                        'Acc (No KG)': f'{100 * val_correct_no_kg / val_total_no_kg:.2f}%'
                    })
            
            
            val_acc_kg = 100 * val_correct_kg / val_total_kg if val_total_kg > 0 else 0
            val_acc_no_kg = 100 * val_correct_no_kg / val_total_no_kg if val_total_no_kg > 0 else 0
            
            # –ü–æ—Å–ª–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è val_acc_kg –∏ val_acc_no_kg –¥–æ–±–∞–≤–∏—Ç—å:

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å KG
            train_prec_kg, train_rec_kg, train_f1_kg_val = calculate_metrics(epoch_train_labels_kg, epoch_train_preds_kg)
            val_prec_kg, val_rec_kg, val_f1_kg_val = calculate_metrics(epoch_val_labels_kg, epoch_val_preds_kg)

            train_precision_kg.append(train_prec_kg)
            train_recall_kg.append(train_rec_kg)
            train_f1_kg.append(train_f1_kg_val)
            val_precision_kg.append(val_prec_kg)
            val_recall_kg.append(val_rec_kg)
            val_f1_kg.append(val_f1_kg_val)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ –±–µ–∑ KG
            train_prec_no_kg, train_rec_no_kg, train_f1_no_kg_val = calculate_metrics(epoch_train_labels_no_kg, epoch_train_preds_no_kg)
            val_prec_no_kg, val_rec_no_kg, val_f1_no_kg_val = calculate_metrics(epoch_val_labels_no_kg, epoch_val_preds_no_kg)

            train_precision_no_kg.append(train_prec_no_kg)
            train_recall_no_kg.append(train_rec_no_kg)
            train_f1_no_kg.append(train_f1_no_kg_val)
            val_precision_no_kg.append(val_prec_no_kg)
            val_recall_no_kg.append(val_rec_no_kg)
            val_f1_no_kg.append(val_f1_no_kg_val)
            
            # –ü–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –¥–æ–±–∞–≤–∏—Ç—å:
            train_losses_kg.append(np.mean(epoch_train_losses_kg))
            val_losses_kg.append(np.mean(epoch_val_losses_kg))
            train_accs_kg.append(100 * train_correct_kg / train_total_kg)
            val_accs_kg.append(val_acc_kg)

            train_losses_no_kg.append(np.mean(epoch_train_losses_no_kg))
            val_losses_no_kg.append(np.mean(epoch_val_losses_no_kg))
            train_accs_no_kg.append(100 * train_correct_no_kg / train_total_no_kg)
            val_accs_no_kg.append(val_acc_no_kg)

            print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (KG): {val_acc_kg:.2f}%")
            print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (KG): P={val_prec_kg:.3f}, R={val_rec_kg:.3f}, F1={val_f1_kg_val:.3f}")
            print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–±–µ–∑ KG): {val_acc_no_kg:.2f}%")
            print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ KG): P={val_prec_no_kg:.3f}, R={val_rec_no_kg:.3f}, F1={val_f1_no_kg_val:.3f}")
            
            if val_acc_kg > best_val_acc_kg:
                best_val_acc_kg = val_acc_kg
                torch.save(model_with_kg.state_dict(), 'best_contradiction_model_kg_11.pth')
            
            if val_acc_no_kg > best_val_acc_no_kg:
                best_val_acc_no_kg = val_acc_no_kg
                torch.save(model_without_kg.state_dict(), 'best_contradiction_model_no_kg_11.pth')
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        print(f"–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (KG): {best_val_acc_kg:.2f}%")
        print(f"–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–±–µ–∑ KG): {best_val_acc_no_kg:.2f}%")
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        try:
            plot_training_curves(train_losses_kg, val_losses_kg, train_accs_kg, val_accs_kg,
                                train_losses_no_kg, val_losses_no_kg, train_accs_no_kg, val_accs_no_kg,
                                train_precision_kg, val_precision_kg, train_recall_kg, val_recall_kg, train_f1_kg, val_f1_kg,
                                train_precision_no_kg, val_precision_no_kg, train_recall_no_kg, val_recall_no_kg, train_f1_no_kg, val_f1_no_kg,
                                'charts/training_curves_detailed.png')
    
            # –°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            final_metrics_kg = [val_accs_kg[-1]/100, val_precision_kg[-1], val_recall_kg[-1], val_f1_kg[-1]]
            final_metrics_no_kg = [val_accs_no_kg[-1]/100, val_precision_no_kg[-1], val_recall_no_kg[-1], val_f1_no_kg[-1]]
            
            plot_metrics_summary(final_metrics_kg, final_metrics_no_kg, 'charts/metrics_summary.png')
    
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
        return model_with_kg, model_without_kg
    
    def evaluate_model(self, model_with_kg, model_without_kg, val_loader):
        """–û—Ü–µ–Ω–∫–∞ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        print("–≠—Ç–∞–ø 6: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π")
        
        model_with_kg.eval()
        model_without_kg.eval()
        all_predictions_kg = []
        all_predictions_no_kg = []
        all_labels = []
        
        with torch.no_grad():
            eval_pbar = tqdm(val_loader, desc="–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π")
            for batch in eval_pbar:
                try:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['label'].to(self.device)
                    kg_embeddings = batch['kg_embedding'].to(self.device)
                    
                    outputs_kg = model_with_kg(input_ids, attention_mask, kg_embeddings)
                    _, predicted_kg = torch.max(outputs_kg.data, 1)
                    all_predictions_kg.extend(predicted_kg.cpu().numpy())
                    
                    outputs_no_kg = model_without_kg(input_ids, attention_mask)
                    _, predicted_no_kg = torch.max(outputs_no_kg.data, 1)
                    all_predictions_no_kg.extend(predicted_no_kg.cpu().numpy())
                    
                    all_labels.extend(labels.cpu().numpy())
                    
                    eval_pbar.set_postfix({
                        '–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ': f'{len(all_predictions_kg)}'
                    })
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –±–∞—Ç—á–∞: {e}")
                    continue
        
        if len(all_predictions_kg) == 0 or len(all_labels) == 0:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            return 0, 0, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏"
        
        accuracy_kg = accuracy_score(all_labels, all_predictions_kg)
        report_kg = classification_report(
            all_labels,
            all_predictions_kg,
            target_names=['SUPPORTS', 'REFUTES', 'NOT_ENOUGH_INFO'],
            zero_division=0
        )
        
        accuracy_no_kg = accuracy_score(all_labels, all_predictions_no_kg)
        report_no_kg = classification_report(
            all_labels,
            all_predictions_no_kg,
            target_names=['SUPPORTS', 'REFUTES', 'NOT_ENOUGH_INFO'],
            zero_division=0
        )
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
        print(f"\n–ú–æ–¥–µ–ª—å —Å KG:")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_kg:.4f}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(all_predictions_kg)}")
        print(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report_kg}")
        
        print(f"\n–ú–æ–¥–µ–ª—å –±–µ–∑ KG:")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_no_kg:.4f}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(all_predictions_no_kg)}")
        print(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report_no_kg}")
        
        print(f"\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
        print(f"–†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ (KG - –±–µ–∑ KG): {(accuracy_kg - accuracy_no_kg):.4f}")
        
        # –í –∫–æ–Ω—Ü–µ –º–µ—Ç–æ–¥–∞ evaluate_model, –ø–æ—Å–ª–µ –≤—Å–µ—Ö print'–æ–≤ –¥–æ–±–∞–≤–∏—Ç—å:

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫
        class_names = ['SUPPORTS', 'REFUTES', 'NOT_ENOUGH_INFO']
        plot_confusion_matrices(all_labels, all_predictions_kg, all_predictions_no_kg,
                            class_names, 'charts/confusion_matrices_comparison.png')

        return accuracy_kg, accuracy_no_kg, f"KG:\n{report_kg}\nNo KG:\n{report_no_kg}"

def check_requirements():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
    
    required_packages = {
        'torch': 'torch',
        'transformers': 'transformers',
        'spacy': 'spacy',
        'sklearn': 'scikit-learn',
        'networkx': 'networkx'
    }
    
    missing_packages = []
    
    for package, install_name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(install_name)
    
    if missing_packages:
        print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–∞–∫–µ—Ç—ã: {missing_packages}")
        print(f"pip install {' '.join(missing_packages)}")
        try:
            import spacy
            try:
                spacy.load('en_core_web_sm')
            except OSError:
                print("–ó–∞–≥—Ä—É–∂–∞–µ–º spacy –º–æ–¥–µ–ª—å...")
                os.system("python -m spacy download en_core_web_sm")
        except ImportError:
            pass
        return False
    
    print("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    return True

def test_fever_data_structure(data_path, num_samples=5):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö FEVER"""
    print(f"–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤ {data_path}...")
    
    if not os.path.exists(data_path):
        print(f"–§–∞–π–ª {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return False
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= num_samples:
                    break
                
                try:
                    item = json.loads(line.strip())
                    print(f"\n–ó–∞–ø–∏—Å—å {i}:")
                    print(f"  –ö–ª—é—á–∏: {list(item.keys())}")
                    print(f"  ID: {item.get('id', '–ù–µ—Ç')}")
                    print(f"  Label: {item.get('label', '–ù–µ—Ç')}")
                    print(f"  Claim: {item.get('claim', '–ù–µ—Ç')[:100]}...")
                    
                    if 'evidence' in item:
                        evidence = item['evidence']
                        print(f"  Evidence —Ç–∏–ø: {type(evidence)}")
                        print(f"  Evidence –¥–ª–∏–Ω–∞: {len(evidence) if isinstance(evidence, (list, str)) else 'N/A'}")
                        
                        if isinstance(evidence, list) and evidence:
                            print(f"  Evidence[0] —Ç–∏–ø: {type(evidence[0])}")
                            if isinstance(evidence[0], list) and evidence[0]:
                                print(f"  Evidence[0] –¥–ª–∏–Ω–∞: {len(evidence[0])}")
                                print(f"  Evidence[0] —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {evidence[0]}")
                    
                    print("-" * 50)
                    
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –≤ —Å—Ç—Ä–æ–∫–µ {i}: {e}")
                    
        print("–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω")
        return True
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("–°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π")
    print("=" * 70)
    
    if not check_requirements():
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        return
    
    data_path = "fever_data/fever_train.jsonl"
    
    if not os.path.exists(data_path):
        print(f"–§–∞–π–ª {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç FEVER –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ.")
        
        possible_paths = [
            "fever_train.jsonl",
            "data/fever_train.jsonl",
            "../fever_data/fever_train.jsonl"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                data_path = path
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {data_path}")
                break
        else:
            return
    
    print("\n" + "="*50)
    if not test_fever_data_structure(data_path):
        return
    
    print("\n" + "="*50)
    pipeline = ContradictionDetectionPipeline(data_path)
    
    try:
        print("–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º datasets –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
        model_with_kg, model_without_kg = pipeline.run_full_pipeline(max_samples=15000)
        
        if model_with_kg is not None and model_without_kg is not None:
            print("\n–ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print("–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
            
            pipeline_info = {
                'num_entities': len(pipeline.entity_extractor.entity_to_id),
                'num_relations': len(pipeline.entity_extractor.relation_to_id),
                'graph_edges': pipeline.entity_extractor.knowledge_graph.number_of_edges(),
                'device_used': str(pipeline.device)
            }
            
            with open('charts/pipeline_info.json', 'w') as f:
                json.dump(pipeline_info, f, indent=2)
            
            print(f"\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–π–ø–ª–∞–π–Ω–µ:")
            print(f"  –°—É—â–Ω–æ—Å—Ç–µ–π –≤ KG: {pipeline_info['num_entities']}")
            print(f"  –û—Ç–Ω–æ—à–µ–Ω–∏–π –≤ KG: {pipeline_info['num_relations']}")
            print(f"  –†—ë–±–µ—Ä –≤ –≥—Ä–∞—Ñ–µ: {pipeline_info['graph_edges']}")
            print(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {pipeline_info['device_used']}")
        else:
            print("–ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {str(e)}")
        import traceback
        print("\n–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ:")
        traceback.print_exc()
        
        print("\n–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")
        print(f"GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name()}")
            print(f"GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

def run_quick_test():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    print("–ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    
    test_data = [
        {
            'id': 1,
            'claim': 'The capital of France is Paris.',
            'evidence': 'Paris is the capital and most populous city of France.',
            'label': 0,
            'label_name': 'SUPPORTS'
        },
        {
            'id': 2,
            'claim': 'The Earth is flat.',
            'evidence': 'The Earth is a sphere.',
            'label': 1,
            'label_name': 'REFUTES'
        }
    ]
    
    try:
        extractor = EntityExtractor()
        entity_pairs = extractor.extract_entities_and_relations(test_data)
        print(f"–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ")
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {len(extractor.entity_to_id)}")
    except Exception as e:
        print(f"–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ—É—Å–ø–µ—à–µ–Ω: {e}")
    
    try:
        model = ContradictionClassifier('bert-base-uncased')
        print(f"–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–µ—É—Å–ø–µ—à–µ–Ω: {e}")
    
    print("–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    main()
